import time, json, csv
from collections import deque
from urllib.parse import urljoin, urldefrag, urlparse
import requests
from bs4 import BeautifulSoup, Comment
import urllib.robotparser as robotparser

# ===== 設定（必要ならここだけ調整）=====
START_URL = "https://www.musashino-u.ac.jp/"
ALLOWED_NETLOC = "www.musashino-u.ac.jp"
RATE_LIMIT_SECONDS = 1.0      # 授業の最低1秒は守ろう
MAX_PAGES = 120               # まずは小さめでテスト
MAX_DEPTH = 4                 # 深さ制限で暴走防止
CONTACT_URL_OR_EMAIL = "https://github.com/ChanChorMiu"  
USER_AGENT = f"MU-Student-Scraper/1.0 (+{CONTACT_URL_OR_EMAIL})"
PRINT_EVERY = 10              # 進捗表示間隔
# =====================================

# 画像/PDFなどは辿らない
BINARY_EXT_BLOCKLIST = {
    ".jpg",".jpeg",".png",".gif",".webp",".bmp",".svg",".ico",
    ".pdf",".doc",".docx",".xls",".xlsx",".ppt",".pptx",
    ".zip",".rar",".7z",".tar",".gz",
    ".mp3",".wav",".ogg",".mp4",".mov",".avi",".mkv",
    ".exe",".dmg",".pkg"
}

def allowed_by_robots(url: str, user_agent: str) -> bool:
    p = urlparse(url)
    robots_url = f"{p.scheme}://{p.netloc}/robots.txt"
    rp = robotparser.RobotFileParser()
    try:
        rp.set_url(robots_url); rp.read()
    except Exception:
        return True
    cd = rp.crawl_delay(user_agent) or rp.crawl_delay("*")
    if cd and cd > RATE_LIMIT_SECONDS:
        # robots.txt に crawl-delay があれば尊重
        globals()["RATE_LIMIT_SECONDS"] = cd
    return rp.can_fetch(user_agent, url)

def is_same_domain(url: str, allowed_netloc: str) -> bool:
    p = urlparse(url)
    return p.netloc == allowed_netloc and p.scheme in {"http","https"}

def normalize_link(base_url: str, link: str) -> str:
    abs_url = urljoin(base_url, link)
    abs_url, _ = urldefrag(abs_url)
    p = urlparse(abs_url)
    # クエリを落として同一ページの重複を減らす
    return f"{p.scheme}://{p.netloc}{p.path}"

def is_binary_url(url: str) -> bool:
    return any(urlparse(url).path.lower().endswith(ext) for ext in BINARY_EXT_BLOCKLIST)

def extract_links(html: str, base_url: str) -> set[str]:
    soup = BeautifulSoup(html, "lxml")
    # コメント内リンクは無視
    for c in soup.find_all(string=lambda t: isinstance(t, Comment)):
        c.extract()
    out = set()
    for a in soup.find_all("a", href=True):
        href = (a.get("href") or "").strip()
        if not href or href.startswith(("mailto:","tel:","javascript:")):
            continue
        u = normalize_link(base_url, href)
        if is_binary_url(u):
            continue
        out.add(u)
    return out

def extract_title(html: str) -> str:
    soup = BeautifulSoup(html, "lxml")
    return (soup.title.string.strip() if soup.title and soup.title.string else "")

def fetch(session: requests.Session, url: str, user_agent: str, rate_limit: float):
    # ← ここを細切れスリープにして Ctrl+C に即反応
    waited = 0.0
    step = 0.1
    while waited < rate_limit:
        time.sleep(step)
        waited += step
    try:
        r = session.get(url, headers={"User-Agent": user_agent}, timeout=20)
        if r.status_code == 200 and "text/html" in r.headers.get("Content-Type",""):
            return r
    except requests.RequestException:
        return None
    return None

def save_partial(sitemap_dict: dict):
    with open("musashino_sitemap.json","w",encoding="utf-8") as f:
        json.dump(sitemap_dict, f, ensure_ascii=False, indent=2)
    with open("musashino_sitemap.csv","w",encoding="utf-8",newline="") as f:
        w = csv.writer(f); w.writerow(["url","title"]); w.writerows(sitemap_dict.items())

def main():
    session = requests.Session()
    if not allowed_by_robots(START_URL, USER_AGENT):
        print("robots.txt により START_URL へのアクセスが禁止されています。")
        return

    visited = set()
    queue = deque([(START_URL, 0)])   # (url, depth)
    sitemap_dict = {}
    pages = 0

    try:
        while queue and pages < MAX_PAGES:
            url, depth = queue.popleft()
            if url in visited:
                continue
            if not is_same_domain(url, ALLOWED_NETLOC):
                continue
            if not allowed_by_robots(url, USER_AGENT):
                continue

            resp = fetch(session, url, USER_AGENT, RATE_LIMIT_SECONDS)
            if resp is None:
                visited.add(url)
                continue

            html = resp.text
            sitemap_dict[url] = extract_title(html)
            visited.add(url); pages += 1

            if pages % PRINT_EVERY == 0 or pages == 1:
                print(f"[{pages}] {url} -> {sitemap_dict[url]!r}")

            if depth < MAX_DEPTH:
                for link in extract_links(html, url):
                    if link not in visited:
                        queue.append((link, depth+1))

    except KeyboardInterrupt:
        print("\n中断を受け取りました。ここまでの結果を保存します…")
    finally:
        print("\n--- 辞書型（URL -> <title>）---")
        print(sitemap_dict)
        save_partial(sitemap_dict)
        print("Saved: musashino_sitemap.json / musashino_sitemap.csv")
        print(f"収集ページ数: {len(sitemap_dict)} / 上限: {MAX_PAGES}, 深さ上限: {MAX_DEPTH}")

if __name__ == "__main__":
    main()
